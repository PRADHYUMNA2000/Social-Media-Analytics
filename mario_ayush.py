# -*- coding: utf-8 -*-
"""Mario_Ayush.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z84tTAMfOwWQqjiVMoEVgP-T2cCoeYe6
"""

import pandas as pd
import snscrape.modules.twitter as sntwitter
import itertools
from textblob import TextBlob
from google.colab import files

import io
mario_c1 = pd.read_csv(io.BytesIO(fifa_ny_la_chic['mario_c1.csv']))

mario_c1=mario_c1.iloc[:,1:]
mario_c1.head(2)

mario=pd.concat([mario_rest,mario])
mario.shape

mario_c1.shape

mario['Location'].value_counts()

"""**Add polarity:**"""

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()

mario['scores'] = mario['Tweet'].apply(lambda review: sid.polarity_scores(str(review)))

mario.head(2)

mario['compound']  = mario['scores'].apply(lambda score_dict: score_dict['compound'])

mario.head(2)

"""**Checking and eliminating null values in data frame**"""

mario_c1=mario[~mario['UserName'].isna()]
 mario_c1.isna().sum()

mario_c1.shape

"""**Removing Duplicates at Overall Level**"""

mario_c1['rank'] = mario_c1.groupby('Tweet')['Location'].rank(method='first')
mario_c1.head(2)

mario_c1=mario_c1[mario_c1['rank']==1]
print(mario_c1.shape)

mario_c1['UserName'].drop_duplicates().shape

mario_c1.head(2)

"""**Creating 4 categories - PP, PN, NP, NN polarity tweets** """

# PP
f1=mario_c1[(mario_c1.compound>0) & (mario_c1.Tweet_Created_Date<'2022-08-01')]
f2=mario_c1[(mario_c1.compound>0) & (mario_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
mario_pos_pos_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
mario_pos_pos_usr.drop_duplicates(inplace=True)
print(mario_pos_pos_usr.head(2))
print(mario_pos_pos_usr.shape)

names=list(mario_pos_pos_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

mario_pos_pos=pd.concat(frames)
print(mario_pos_pos.shape)

# PN
f1=mario_c1[(mario_c1.compound>0) & (mario_c1.Tweet_Created_Date<'2022-08-01')]
f2=mario_c1[(mario_c1.compound<0) & (mario_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
mario_pos_neg_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
mario_pos_neg_usr.drop_duplicates(inplace=True)
print(mario_pos_neg_usr.head(2))
print(mario_pos_neg_usr.shape)

names=list(mario_pos_neg_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

mario_pos_neg=pd.concat(frames)
print(mario_pos_neg.shape)

# NN
f1=mario_c1[(mario_c1.compound<0) & (mario_c1.Tweet_Created_Date<'2022-08-01')]
f2=mario_c1[(mario_c1.compound<0) & (mario_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
mario_neg_neg_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
mario_neg_neg_usr.drop_duplicates(inplace=True)
print(mario_neg_neg_usr.head(2))
print(mario_neg_neg_usr.shape)

names=list(mario_neg_neg_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

mario_neg_neg=pd.concat(frames)
print(mario_neg_neg.shape)

# NP
f1=mario_c1[(mario_c1.compound<0) & (mario_c1.Tweet_Created_Date<'2022-08-01')]
f2=mario_c1[(mario_c1.compound>0) & (mario_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
mario_neg_pos_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
mario_neg_pos_usr.drop_duplicates(inplace=True)
print(mario_neg_pos_usr.head(2))
print(mario_neg_pos_usr.shape)

names=list(mario_neg_pos_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

mario_neg_pos=pd.concat(frames)
print(mario_neg_pos.shape)

#Final Population for Novel:
 
mario_neg=pd.concat([mario_neg_pos])
print(mario_final.shape)
print(mario_final.head(2))

#Final Population for Novel:
 
mario_final=pd.concat([mario_neg_pos,mario_pos_pos])
print(mario_final.shape)
print(mario_final.head(2))

mario_final.shape

f1=mario_final.groupby(by='UserName').Tweet_Created_Date.count()
f1=pd.DataFrame(f1)
f1.reset_index(inplace=True)
f1 = f1.rename(columns = {'index':'UserName','Tweet_Created_Date':'Tweets'})
print(f1.head(2))
print(f1.shape)

f2=mario_final.groupby(by='UserName').agg({'Tweet_Created_Date' :'max', 'compound' : 'max'})
f2.columns = ['max_date','maxpolarity']
f2=f2.reset_index()
print(f2.head(2))

f3=f1.merge(f2,how='left',on='UserName')
print(f3.head(2))
print(f3.shape)

f20=mario_c1[['UserName','Location']].drop_duplicates()
f20['rank'] = f20.groupby('UserName')['Location'].rank(method='first')
f20=f20[f20['rank']==1]
f20.shape

mario_final=f3.merge(f20[['UserName','Location']],how='left', on='UserName')
print(mario_final.head(2))
print(mario_final.shape)

"""**Additional Attributes - User Followers, Joining date and description**"""

import tweepy
  
# assign the values accordingly
consumer_key = 'PNVeKusZOpo3XTLlvH0k2oHtQ'
consumer_secret = 'XksFog5efD9UIKR4n2bbrpg38lloUW7SKnxAsFd7ZeEdYe4FJF'
access_token = '1020155065653329920-bG6SCYIn5K1PwbOHNi3nSg2hL0NGEo'
access_token_secret = '0kRj7RfSWymKgeLle7DAcOdUoQxwghRRMVsTIljDhQ3rB'
  
# authorization of consumer key and consumer secret
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
  
# set access to user's access key and access secret 
auth.set_access_token(access_token, access_token_secret)
  
# calling the api 
api = tweepy.API(auth,wait_on_rate_limit=True)

# Manual
import time
f7={}
d7={}
usernames=list_rest.UserName.values[0:809]
for j in usernames:
  try:
    user = api.get_user(j)
    #usercreatedat[j]=user.created_at
    f7[j]=user.followers_count
    d7[j]=user.description
  except:
    pass
#print(usercreatedat)
print(len(f7))
print(len(d7))

f7

d7_df=pd.DataFrame.from_dict(d7,orient='index')
d7_df.reset_index(inplace=True)
d7_df = d7_df.rename(columns = {'index':'UserName', '0': 'UserDesc'})
d7_df.head(2)

usr_desc=pd.concat([d1_df,d2_df,d3_df,d4_df,d5_df,d6_df,d7_df])
usr_desc.drop_duplicates(inplace=True)
print(usr_desc.head(2))
print(usr_desc.shape)

usr_foll=pd.concat([f7_df])
usr_foll.drop_duplicates(inplace=True)
print(usr_foll.head(2))
print(usr_foll.shape)

novel_final_v1=novel_final.merge(usr_desc,how='left',on='UserName')
print(novel_final_v1.head(2))
print(novel_final_v1.shape)

novel_final_v1.columns=['UserName', 'Tweets', 'max_date', 'maxpolarity', 'Location', 'Followers',
       'User_Desc']

novel_final_v1.Tweets.sum()

novel_final_v1['Tweets_perc']=((novel_final_v1['Tweets']*100)/45935).round(2)
novel_final_v1.head(2)

"""**Csv download using dataframe 'fifa'**"""

keyword='mario_neg_neg'
mario_neg_neg.to_csv('{}.csv'.format(keyword))
files.download('{}.csv'.format(keyword))

from google.colab import files
fifa_ny_la_chic = files.upload()

import io
mario_c1 = pd.read_csv(io.BytesIO(fifa_ny_la_chic['mario_c1.csv']))

mario_c1=mario_c1.iloc[:,1:]
mario_c1.head(2)