# -*- coding: utf-8 -*-
"""SamsungS23_Ayush.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yqYkvTcgeaZz_0hc0PmZ2LjDySr-hA9n
"""

import pandas as pd
#import snscrape.modules.twitter as sntwitter
import itertools
from textblob import TextBlob
from google.colab import files

samsung = pd.concat([sms_ny_v1,sms_LA_v1,sms_Chic_v1,sms_SA_v1,sms_SD_v1,sms_Ind_v1,sms_Col_v1,sms_FW_v1,
                   sms_Austin_v1,sms_Jack_v1,sms_SJ_v1,sms_Dallas_v1,sms_Bronx_v1], axis=0)
samsung.shape

samsung['Location'].value_counts()

keyword='samsung'
samsung.to_csv('{}.csv'.format(keyword))
files.download('{}.csv'.format(keyword))

from google.colab import files
fifa_ny_la_chic = files.upload()

import io
samsung_rest = pd.read_csv(io.BytesIO(fifa_ny_la_chic['samsung_rest.csv']))

samsung_rest=samsung_rest.iloc[:,1:]
samsung_rest.head(2)

samsung=pd.concat([samsung_rest,samsung])
samsung.shape

samsung.head(2)

samsung['Location'].value_counts()

"""**Add polarity:**"""

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer

sid = SentimentIntensityAnalyzer()

samsung['scores'] = samsung['Tweet'].apply(lambda review: sid.polarity_scores(str(review)))

samsung.head(2)

samsung['compound']  = samsung['scores'].apply(lambda score_dict: score_dict['compound'])

samsung.head(2)

samsung.Tweet_Created_Date.head(2)

"""**Checking and eliminating null values in data frame**"""

samsung.shape

samsung_c1=samsung[~samsung['UserName'].isna()]
 samsung_c1.isna().sum()

samsung_c1.shape

"""**Removing Duplicates at Overall Level**"""

samsung_c1['rank'] = samsung_c1.groupby('Tweet')['Location'].rank(method='first')
samsung_c1.head(2)

samsung_c1=samsung_c1[samsung_c1['rank']==1]
print(samsung_c1.shape)

samsung_c1['UserName'].drop_duplicates().shape

"""**Creating 4 categories - PP, PN, NP, NN polarity tweets** """

# PP
f1=samsung_c1[(samsung_c1.compound>0) & (samsung_c1.Tweet_Created_Date<'2022-08-01')]
f2=samsung_c1[(samsung_c1.compound>0) & (samsung_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
sms_pos_pos_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
sms_pos_pos_usr.drop_duplicates(inplace=True)
print(sms_pos_pos_usr.head(2))
print(sms_pos_pos_usr.shape)

names=list(sms_pos_pos_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

sms_pos_pos=pd.concat(frames)
print(sms_pos_pos.shape)

# PN
f1=samsung_c1[(samsung_c1.compound>0) & (samsung_c1.Tweet_Created_Date<'2022-08-01')]
f2=samsung_c1[(samsung_c1.compound<0) & (samsung_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
sms_pos_neg_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
sms_pos_neg_usr.drop_duplicates(inplace=True)
print(sms_pos_neg_usr.head(2))
print(sms_pos_neg_usr.shape)

names=list(sms_pos_neg_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

sms_pos_neg=pd.concat(frames)
print(sms_pos_neg.shape)

# NN
f1=samsung_c1[(samsung_c1.compound<0) & (samsung_c1.Tweet_Created_Date<'2022-08-01')]
f2=samsung_c1[(samsung_c1.compound<0) & (samsung_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
sms_neg_neg_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
sms_neg_neg_usr.drop_duplicates(inplace=True)
print(sms_neg_neg_usr.head(2))
print(sms_neg_neg_usr.shape)

names=list(sms_neg_neg_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

sms_neg_neg=pd.concat(frames)
print(sms_neg_neg.shape)

# NP
f1=samsung_c1[(samsung_c1.compound<0) & (samsung_c1.Tweet_Created_Date<'2022-08-01')]
f2=samsung_c1[(samsung_c1.compound>0) & (samsung_c1.Tweet_Created_Date>='2022-08-01')]
f3=pd.DataFrame(f1['UserName'])
sms_neg_pos_usr=f3.merge(f2['UserName'],how='inner',on='UserName')
sms_neg_pos_usr.drop_duplicates(inplace=True)
print(sms_neg_pos_usr.head(2))
print(sms_neg_pos_usr.shape)

names=list(sms_neg_pos_usr['UserName'])
f1=f1[f1['UserName'].isin(names)]
f2=f2[f2['UserName'].isin(names)]
frames = [f1,f2]

sms_neg_pos=pd.concat(frames)
print(sms_neg_pos.shape)

#Final Population for Samsung Galaxy S23:
 
sms_final=pd.concat([sms_neg_pos,sms_pos_pos])
print(sms_final.shape)
print(sms_final.head(2))

f1=sms_final.groupby(by='UserName').Tweet_Created_Date.count()
f1=pd.DataFrame(f1)
f1.reset_index(inplace=True)
f1 = f1.rename(columns = {'index':'UserName','Tweet_Created_Date':'Tweets'})
print(f1.head(2))
print(f1.shape)

f2=sms_final.groupby(by='UserName').agg({'Tweet_Created_Date' :'max', 'compound' : 'max'})
f2.columns = ['max_date','maxpolarity']
f2=f2.reset_index()
print(f2.head(2))

f3=f1.merge(f2,how='left',on='UserName')
print(f3.head(2))
print(f3.shape)

f20=samsung_c1[['UserName','Location']].drop_duplicates()
f20['rank'] = f20.groupby('UserName')['Location'].rank(method='first')
f20=f20[f20['rank']==1]
f20.shape

sms_final=f3.merge(f20[['UserName','Location']],how='left', on='UserName')
print(sms_final.head(2))
print(sms_final.shape)

"""*For word cloud:*"""

abc=samsung_c1[['Tweet','Location','compound']].drop_duplicates()
print(abc.shape)
print(samsung_c1[['Tweet','Location','compound']].shape)
samsung_neg_wc=samsung_c1[['Tweet','Location','compound']]
samsung_neg_wc=samsung_neg_wc[samsung_neg_wc['compound']<0]
print(samsung_neg_wc.shape)

"""**Additional Attributes - User Followers, Joining date and description**"""

import tweepy
  
# assign the values accordingly
consumer_key = 'PNVeKusZOpo3XTLlvH0k2oHtQ'
consumer_secret = 'XksFog5efD9UIKR4n2bbrpg38lloUW7SKnxAsFd7ZeEdYe4FJF'
access_token = '1020155065653329920-bG6SCYIn5K1PwbOHNi3nSg2hL0NGEo'
access_token_secret = '0kRj7RfSWymKgeLle7DAcOdUoQxwghRRMVsTIljDhQ3rB'
  
# authorization of consumer key and consumer secret
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
  
# set access to user's access key and access secret 
auth.set_access_token(access_token, access_token_secret)
  
# calling the api 
api = tweepy.API(auth,wait_on_rate_limit=True)

import time
f7={}
d7={}
usernames=sms_final.UserName.values[0:1000]
for j in usernames:
  try:
    user = api.get_user(j)
    #usercreatedat[j]=user.created_at
    f7[j]=user.followers_count
    d7[j]=user.description
  except:
    pass
#print(usercreatedat)
print(len(f7))
print(len(d7))

sms_final.shape

d1_df=pd.DataFrame.from_dict(d1,orient='index')
d1_df.reset_index(inplace=True)
d1_df = d1_df.rename(columns = {'index':'UserName', '0': 'UserDesc'})
d1_df.head(2)

usr_foll=pd.concat([f1_df,f2_df,f3_df,f4_df,f5_df,f6_df,f7_df])
usr_foll.drop_duplicates(inplace=True)
print(usr_foll.head(2))
print(usr_foll.shape)

sms_final_v1=sms_final_v1.merge(usr_desc,how='left',on='UserName')
print(sms_final_v1.head(2))
print(sms_final_v1.shape)

sms_final_v1.columns=['UserName', 'Tweets', 'max_date', 'maxpolarity', 'Location', 'Followers',
       'User_Desc']

sms_final_v1['Tweets_perc']=((sms_final_v1['Tweets']*100)/34647).round(2)
sms_final_v1.head(2)

"""**Csv download using dataframe 'fifa'**"""

keyword='trend_analysis_samsung'
samsung_c1.to_csv('{}.csv'.format(keyword))
files.download('{}.csv'.format(keyword))

"""*Visualize tweets month wise*"""

import datetime
samsung_c1['month']=pd.to_datetime(samsung_c1['Date']).dt.month
samsung_c1['year']=pd.to_datetime(samsung_c1['Date']).dt.year
samsung_c1['period']=samsung_c1['month'].astype(str) + "-" + samsung_c1['year'].astype(str)
samsung_c1['Tweet_Date'] = pd.to_datetime(samsung_c1['Date']).dt.date
samsung_c1.head(2)

samsung_c1.head(2)

"""*Time Series Trend on Sentiments*"""

import numpy as np
conditions = [
    (samsung_c1['compound'] > 0),
    (samsung_c1['compound'] ==0),
    (samsung_c1['compound'] < 0)
    ]
values = ['Positive', 'Neutral', 'Negative']
samsung_c1['Sentiment'] = np.select(conditions, values)
samsung_c1['month']=pd.to_datetime(samsung_c1['Tweet_Created_Date']).dt.month
samsung_c1.head(2)

sms_trend=samsung_c1.groupby(by=['month','Sentiment']).compound.mean()
sms_trend=pd.DataFrame(sms_trend)
sms_trend.reset_index(inplace=True)
#fifa_trend = fifa_trend.rename(columns = {'index':'UserName','Tweet_Created_Date':'Tweets'})
print(sms_trend.head(5))
print(sms_trend.shape)

sms_trend_pos=sms_trend[sms_trend['Sentiment']=='Positive']
sms_trend_neg=sms_trend[sms_trend['Sentiment']=='Negative']
print(sms_trend_pos.shape)
print(sms_trend_neg.shape)

import seaborn as sns
import numpy as np
from matplotlib import pyplot as plt
plt.rcParams["figure.figsize"] = [14, 4]
plt.rcParams["figure.autolayout"] = True
f, axes = plt.subplots(1, 2)
sns.lineplot(data=sms_trend_pos,x='month',y='compound',hue='Sentiment',ax=axes[0])
sns.lineplot(data=sms_trend_neg,x='month',y='compound',hue='Sentiment',ax=axes[1])
plt.show()